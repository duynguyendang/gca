package meb

import (
	"context"
	"fmt"
	"log/slog"
	"regexp"
	"strings"
	"unicode"

	"github.com/duynguyendang/gca/pkg/datalog"
	"github.com/duynguyendang/gca/pkg/meb/keys"

	"github.com/dgraph-io/badger/v4"
)

// === store.KnowledgeStore implementation ===

// AddFact inserts a single fact into the knowledge base.
func (m *MEBStore) AddFact(fact Fact) error {
	// Validate the fact
	if err := validateFact(fact); err != nil {
		return fmt.Errorf("failed to add fact: %w", err)
	}

	// Use AddFactBatch for consistency with quad store format
	return m.AddFactBatch([]Fact{fact})
}

// AddFactBatch inserts multiple facts in a single operation using quad indices.
// Uses batch dictionary encoding for optimal performance.
// Populates 2 indices: SPO (forward), OPS (reverse).
func (m *MEBStore) AddFactBatch(facts []Fact) error {
	// Validate all facts first
	if err := validateFacts(facts); err != nil {
		return fmt.Errorf("batch validation failed: %w", err)
	}

	// 1. Collect all UNIQUE strings that need encoding
	type stringRef struct {
		index int  // Index in uniqueStrings
		isObj bool // True if this is an object string
	}
	factStringRefs := make([][]stringRef, len(facts))
	uniqueStringsMap := make(map[string]int) // string -> index in uniqueStrings
	var uniqueStrings []string

	for i, fact := range facts {
		// Normalize graph to "default" if empty
		graph := normalizeGraph(fact.Graph)

		// Process Subject
		if _, ok := uniqueStringsMap[string(fact.Subject)]; !ok {
			uniqueStringsMap[string(fact.Subject)] = len(uniqueStrings)
			uniqueStrings = append(uniqueStrings, string(fact.Subject))
		}
		factStringRefs[i] = append(factStringRefs[i], stringRef{index: uniqueStringsMap[string(fact.Subject)], isObj: false})

		// Process Predicate
		if _, ok := uniqueStringsMap[fact.Predicate]; !ok {
			uniqueStringsMap[fact.Predicate] = len(uniqueStrings)
			uniqueStrings = append(uniqueStrings, fact.Predicate)
		}
		factStringRefs[i] = append(factStringRefs[i], stringRef{index: uniqueStringsMap[fact.Predicate], isObj: false})

		// Process Graph
		if _, ok := uniqueStringsMap[graph]; !ok {
			uniqueStringsMap[graph] = len(uniqueStrings)
			uniqueStrings = append(uniqueStrings, graph)
		}
		factStringRefs[i] = append(factStringRefs[i], stringRef{index: uniqueStringsMap[graph], isObj: false})

		// Process Object if it's a string
		if s, ok := fact.Object.(string); ok {
			if _, ok := uniqueStringsMap[s]; !ok {
				uniqueStringsMap[s] = len(uniqueStrings)
				uniqueStrings = append(uniqueStrings, s)
			}
			factStringRefs[i] = append(factStringRefs[i], stringRef{index: uniqueStringsMap[s], isObj: true})
		}
	}

	// 2. Batch encode all UNIQUE strings to IDs (single call, minimal locking)
	ids, err := m.dict.GetIDs(uniqueStrings)
	if err != nil {
		return fmt.Errorf("failed to encode strings: %w", err)
	}

	// 3. Build BadgerDB batch using pre-encoded IDs (pure RAM operations)
	batch := m.db.NewWriteBatch()
	defer batch.Cancel()

	for i, fact := range facts {
		// Get IDs for Subject, Predicate, Graph from refs
		sID := ids[factStringRefs[i][0].index]
		pID := ids[factStringRefs[i][1].index]
		// gID := ids[factStringRefs[i][2].index] // Graph ID unused in 24-byte key mode

		// Handle Object (could be string or other type)
		var oID uint64

		if len(factStringRefs[i]) > 3 && factStringRefs[i][3].isObj {
			// Object is a string, use the ID from refs
			oID = ids[factStringRefs[i][3].index]
		} else {
			// Object is not a string, need to encode it
			_, oID, err = m.encodeObject(fact.Object)
			if err != nil {
				return fmt.Errorf("failed to encode object for fact %d: %w", i, err)
			}
		}

		// Encode Metadata (Weight/Source)
		metaBytes := EncodeFactMetadata(fact)

		// Add to main index: SPO (25 bytes)
		spogKey := keys.EncodeSPOKey(sID, pID, oID)
		if err := batch.Set(spogKey, metaBytes); err != nil {
			return fmt.Errorf("failed to set SPO key for fact %d: %w", i, err)
		}

		// Add to inverse index: OPS (25 bytes)
		// We store metadata in OPS as well for fast retrieval during reverse lookups?
		// Metadata is property of the Fact (edge), so it should be same.
		opsKey := keys.EncodeOPSKey(sID, pID, oID)
		if err := batch.Set(opsKey, metaBytes); err != nil {
			return fmt.Errorf("failed to set OPS key for fact %d: %w", i, err)
		}

		// Add to predicate index: PSO (25 bytes)
		psoKey := keys.EncodePSOKey(sID, pID, oID)
		if err := batch.Set(psoKey, metaBytes); err != nil {
			return fmt.Errorf("failed to set PSO key for fact %d: %w", i, err)
		}

		// Update fact count (zero-cost atomic operation)
		m.numFacts.Add(1)
	}

	if err := batch.Flush(); err != nil {
		return fmt.Errorf("failed to flush batch: %w", err)
	}

	return nil
}

// DeleteGraph removes all facts belonging to the specified graph context.
// Uses batched deletion for memory efficiency with graphs containing millions of facts.
func (m *MEBStore) DeleteGraph(graph string) error {
	// Normalize graph name
	graph = normalizeGraph(graph)

	slog.Info("deleting graph", "graph", graph)

	// Get graph ID
	gID, err := m.dict.GetID(graph)
	if err != nil {
		// Graph doesn't exist, nothing to delete
		slog.Debug("graph not found, nothing to delete", "graph", graph)
		return nil
	}

	// First pass: collect all GSPO keys to delete
	txn := m.db.NewTransaction(false)

	prefix := keys.EncodeQuadGSPOPrefix(gID)

	opts := badger.DefaultIteratorOptions
	opts.PrefetchValues = false

	it := txn.NewIterator(opts)

	type quadKeys struct {
		gspo []byte
		spog []byte
		posg []byte
	}

	const maxDeleteBatchSize = 1000
	var keysToDelete []quadKeys

	for it.Seek(prefix); it.ValidForPrefix(prefix); it.Next() {
		item := it.Item()
		key := item.Key()

		// Decode the quad key
		s, p, o, g := keys.DecodeQuadKey(key)

		// Copy the GSPO key (make a new byte slice)
		gspoKey := make([]byte, len(key))
		copy(gspoKey, key)

		// Generate all three keys
		spogKey := keys.EncodeQuadKey(keys.QuadSPOGPrefix, s, p, o, g)
		posgKey := keys.EncodeQuadKey(keys.QuadPOSGPrefix, s, p, o, g)

		keysToDelete = append(keysToDelete, quadKeys{
			gspo: gspoKey,
			spog: spogKey,
			posg: posgKey,
		})
	}
	it.Close()
	txn.Discard()

	slog.Debug("collected keys for deletion", "count", len(keysToDelete))

	if len(keysToDelete) == 0 {
		slog.Info("no facts found in graph", "graph", graph)
		return nil
	}

	// Second pass: delete all keys in batches
	totalDeleted := 0
	for i := 0; i < len(keysToDelete); i += maxDeleteBatchSize {
		end := i + maxDeleteBatchSize
		if end > len(keysToDelete) {
			end = len(keysToDelete)
		}

		batch := keysToDelete[i:end]

		deleteTxn := m.db.NewTransaction(true)
		for _, keys := range batch {
			if err := deleteTxn.Delete(keys.spog); err != nil {
				slog.Error("failed to delete SPOG key", "error", err)
				deleteTxn.Discard()
				return fmt.Errorf("failed to delete SPOG key: %w", err)
			}
			if err := deleteTxn.Delete(keys.posg); err != nil {
				slog.Error("failed to delete POSG key", "error", err)
				deleteTxn.Discard()
				return fmt.Errorf("failed to delete POSG key: %w", err)
			}
			if err := deleteTxn.Delete(keys.gspo); err != nil {
				slog.Error("failed to delete GSPO key", "error", err)
				deleteTxn.Discard()
				return fmt.Errorf("failed to delete GSPO key: %w", err)
			}
			// Update fact count (zero-cost atomic operation)
			m.numFacts.Add(^uint64(0)) // Atomic decrement
		}

		// Commit the transaction
		if err := deleteTxn.Commit(); err != nil {
			slog.Error("failed to commit delete transaction", "error", err)
			return fmt.Errorf("failed to commit delete batch: %w", err)
		}

		totalDeleted += len(batch)
	}

	slog.Info("graph deleted successfully", "graph", graph, "factsDeleted", totalDeleted)
	return nil
}

// DeleteFactsBySubject removes all facts where the subject matches the given string.
// Used for incremental updates to clear old file facts.
func (m *MEBStore) DeleteFactsBySubject(subject string) error {
	sID, err := m.dict.GetID(subject)
	if err != nil {
		// Subject not found, nothing to delete
		return nil
	}

	// SPO Prefix scan for this subject
	prefix := keys.EncodeSPOPrefix(sID, 0)

	// Collect keys to delete
	var keysToDelete [][]byte

	err = m.db.View(func(txn *badger.Txn) error {
		opts := badger.DefaultIteratorOptions
		opts.PrefetchValues = false
		it := txn.NewIterator(opts)
		defer it.Close()

		for it.Seek(prefix); it.ValidForPrefix(prefix); it.Next() {
			item := it.Item()
			k := item.KeyCopy(nil)
			keysToDelete = append(keysToDelete, k)
		}
		return nil
	})
	if err != nil {
		return fmt.Errorf("failed to scan subject keys: %w", err)
	}

	if len(keysToDelete) == 0 {
		return nil
	}

	// Delete in batch
	wb := m.db.NewWriteBatch()
	defer wb.Cancel()

	count := 0
	for _, spoKey := range keysToDelete {
		s, p, o := keys.DecodeSPOKey(spoKey)

		// Delete SPO
		if err := wb.Delete(spoKey); err != nil {
			return err
		}

		// Delete OPS
		opsKey := keys.EncodeOPSKey(s, p, o)
		if err := wb.Delete(opsKey); err != nil {
			return err
		}

		// Delete PSO
		psoKey := keys.EncodePSOKey(s, p, o)
		if err := wb.Delete(psoKey); err != nil {
			return err
		}

		// Decrement count
		m.numFacts.Add(^uint64(0))
		count++
	}

	if err := wb.Flush(); err != nil {
		return fmt.Errorf("failed to flush delete batch: %w", err)
	}

	slog.Debug("deleted facts for subject", "subject", subject, "count", count)
	return nil
}

// Query executes a Datalog query and returns results.
// Implements Multi-Atom Nested Loop Join.
func (m *MEBStore) Query(ctx context.Context, query string) ([]map[string]any, error) {
	// Parse the query into atoms
	atoms, err := datalog.Parse(query)
	if err != nil {
		return nil, err
	}

	// Step 1: Classification
	var dataAtoms []datalog.Atom
	var constraints []datalog.Atom

	for _, atom := range atoms {
		// Currently only "triples" is supported as a data atom
		// Everything else (like "regex", "neq") is treated as a constraint
		if atom.Predicate == "triples" {
			dataAtoms = append(dataAtoms, atom)
		} else {
			constraints = append(constraints, atom)
		}
	}

	if len(dataAtoms) == 0 {
		return nil, fmt.Errorf("query must contain at least one data atom (e.g. triples(...))")
	}

	// Step 2: Initialize Pipeline
	// Start with one empty binding
	bindings := []map[string]any{make(map[string]any)}

	// Step 3: Sequential Processing (Nested Loop Join)
	for _, atom := range dataAtoms {
		var nextBindings []map[string]any

		for _, binding := range bindings {
			// Substitute known variables into the DataAtom
			// triples(S, P, O) -> check if S, P, O are bound in 'binding'
			// If bound, use the value. If not, it's a variable or constant.

			// Check args count
			if len(atom.Args) != 3 {
				return nil, fmt.Errorf("triples predicate requires 3 arguments, got %d", len(atom.Args))
			}

			// Prepare Scan arguments
			scanArgs := make([]string, 3)
			vars := make(map[int]string) // Index -> Variable Name

			for i, arg := range atom.Args {
				// Datalog convention: variables start with ? or Uppercase letter
				isVar := strings.HasPrefix(arg, "?") || arg == "_" || (len(arg) > 0 && unicode.IsUpper([]rune(arg)[0]))

				if isVar {
					// Check if already bound in current binding
					if val, ok := binding[arg]; ok {
						// Bound variable -> treat as constant for this scan
						scanArgs[i] = fmt.Sprintf("%v", val)
					} else {
						// Unbound variable -> Scan wildcard
						scanArgs[i] = ""
						vars[i] = arg // Track for extraction
					}
				} else {
					// Constant -> use as is
					scanArgs[i] = strings.Trim(arg, "'\"")
				}
			}

			// Graph context (defaulting to all graphs or specific if we add that support)
			// For now, let's scan all known graphs or default.
			// The original implementation scanned specific graphs.
			// Let's iterate a set of "common" graphs for now as per original code behavior?
			// Or better: The original Scan(g="") implementation might not strict enough if we don't have GSPO index support for wildcard graph.
			// User request didn't specify graph handling changes, but "Scan" supports iterating if g is empty IF we have proper indexing.
			// Let's stick to scanning "default" plus others if needed.
			// Ideally, we should scan all graphs.
			// For this implementation, we'll iterate known graphs like in main.go example to be safe, or just "default".
			// Let's assume "default" for now to keep it simple, or iterate a fixed list.
			scanGraphs := []string{"default", "doc1", "doc2"} // TODO: Discover graphs dynamically

			for _, g := range scanGraphs {
				// Scan: Use the partially-bound Atom
				for fact, err := range m.Scan(scanArgs[0], scanArgs[1], scanArgs[2], g) {
					if err != nil {
						// Error scanning (e.g. not found), skip
						continue
					}

					// Expand: Create new binding
					newBinding := make(map[string]any)
					// Copy existing bindings
					for k, v := range binding {
						newBinding[k] = v
					}

					// Extract new bindings from fact
					row := []string{string(fact.Subject), fact.Predicate, ""}
					// Object type handling
					if s, ok := fact.Object.(string); ok {
						row[2] = s
					} else {
						row[2] = fmt.Sprintf("%v", fact.Object)
					}

					for idx, varName := range vars {
						if varName != "_" {
							newBinding[varName] = row[idx]
						}
					}

					// Inject Metadata (Weight/Source) into hidden fields
					newBinding["_weight"] = fact.Weight
					newBinding["_source"] = fact.Source

					nextBindings = append(nextBindings, newBinding)
				}
			}
		}
		// Move to next stage
		bindings = nextBindings
		if len(bindings) == 0 {
			break // Short-circuit if no results
		}
	}

	// Step 4: Post-Filter (Constraints)
	var finalResults []map[string]any
	for _, row := range bindings {
		keep := true
	ConstraintLoop:
		for _, c := range constraints {
			switch c.Predicate {
			case "regex":
				// Format: regex(Var, "pattern")
				if len(c.Args) != 2 {
					return nil, fmt.Errorf("regex constraint requires 2 arguments")
				}
				varName := c.Args[0]
				pattern := strings.Trim(c.Args[1], "\"'")

				val, ok := row[varName]
				if !ok {
					// If the variable used in regex is NOT bound, it fails?
					// Or do we ignore? Constraint implication: strict filter.
					keep = false
					break ConstraintLoop
				}
				valStr := fmt.Sprintf("%v", val)

				matched, err := regexp.MatchString(pattern, valStr)
				if err != nil {
					return nil, fmt.Errorf("invalid regex pattern '%s': %w", pattern, err)
				}
				if !matched {
					keep = false
					break ConstraintLoop
				}
			case "neq":
				// Format: neq(A, B) or A != B
				if len(c.Args) != 2 {
					return nil, fmt.Errorf("neq constraint requires 2 arguments")
				}
				lhsArg := c.Args[0]
				rhsArg := c.Args[1]

				// Helper to resolve value: either variable look up or constant
				resolveVal := func(arg string) string {
					// Check if variable (Uppercase/?)
					isVar := strings.HasPrefix(arg, "?") || arg == "_" || (len(arg) > 0 && unicode.IsUpper([]rune(arg)[0]))
					if isVar {
						if val, ok := row[arg]; ok {
							return fmt.Sprintf("%v", val)
						}
						return "" // Unbound variable?
					}
					// Constant
					return strings.Trim(arg, "\"'")
				}

				valA := resolveVal(lhsArg)
				valB := resolveVal(rhsArg)

				if valA == valB {
					keep = false
					break ConstraintLoop
				}
			default:
				return nil, fmt.Errorf("unknown constraint predicate: %s", c.Predicate)
			}
		}

		if keep {
			finalResults = append(finalResults, row)
		}
	}

	return finalResults, nil
}
